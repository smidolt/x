{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VLM Demo (Google Colab)\n",
        "\n",
        "Clones the repo, installs dependencies, runs VLM blocks + reasoner on a sample image.\n",
        "\n",
        "**Repo:** https://github.com/smidolt/x.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repo\n",
        "!rm -rf /content/x && git clone https://github.com/smidolt/x.git /content/x\n",
        "%cd /content/x\n",
        "\n",
        "# Install deps (CPU-friendly, adjust if GPU available)\n",
        "!pip install -r requirements.txt -r requirements-vlm.txt\n",
        "\n",
        "# Optional: ensure tesseract is available (Colab usually has it)\n",
        "!apt-get update && apt-get install -y tesseract-ocr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example input: use existing sample or upload your own\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "sample = Path('input/google.jpg')\n",
        "if not sample.exists():\n",
        "    print(\"Sample not found, please upload an image to /content/x/input/\")\n",
        "else:\n",
        "    print(\"Using sample:\", sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run VLM orchestrator (preprocess -> OCR -> VLM blocks -> VLM reasoner)\n",
        "# WARNING: VLM model is heavy; ensure runtime has GPU and enough RAM.\n",
        "import subprocess, sys\n",
        "\n",
        "cmd = [\n",
        "    sys.executable, \"-m\", \"src.orchestrator_vlm\",\n",
        "    \"--input\", \"input/google.jpg\",\n", 
        "    \"--output\", \"output_vlm\",\n",
        "    \"--vlm-model-reasoner\", \"Qwen/Qwen2-VL-2B-Instruct\",\n",
        "    \"--vlm-backend-blocks\", \"heuristic\",\n",
        "    \"--vlm-device\", \"auto\",\n",
        "    \"--vlm-max-tokens\", \"128\",\n",
        "    \"--vlm-temperature\", \"0.1\",\n",
        "]\n",
        "\n",
        "print(\"Running:\", \" \".join(cmd))\n",
        "subprocess.run(cmd, check=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect results\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "summary = Path('output_vlm/summary_vlm_orchestrator.json')\n",
        "if summary.exists():\n",
        "    print(summary.read_text()[:1000])\n",
        "else:\n",
        "    print(\"No summary found; check if the orchestrator ran successfully.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
