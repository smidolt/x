{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fed2c69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'cuda', 'index': 0, 'name': 'NVIDIA A100-SXM4-40GB', 'capability': (8, 0)}\n",
      "{'type': 'cpu', 'index': 0, 'name': 'CPU'}\n",
      "Default tensor device: cpu\n",
      "Chosen device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device diagnostics (CPU/CUDA/MPS)\n",
    "import torch\n",
    "\n",
    "def available_devices():\n",
    "    devices = []\n",
    "    if torch.cuda.is_available():\n",
    "        for idx in range(torch.cuda.device_count()):\n",
    "            name = torch.cuda.get_device_name(idx)\n",
    "            cap = torch.cuda.get_device_capability(idx)\n",
    "            devices.append({\"type\": \"cuda\", \"index\": idx, \"name\": name, \"capability\": cap})\n",
    "    if hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        devices.append({\"type\": \"mps\", \"index\": 0, \"name\": \"Apple MPS\"})\n",
    "    devices.append({\"type\": \"cpu\", \"index\": 0, \"name\": \"CPU\"})\n",
    "    return devices\n",
    "\n",
    "for d in available_devices():\n",
    "    print(d)\n",
    "\n",
    "x = torch.tensor([1.0])\n",
    "print(\"Default tensor device:\", x.device)\n",
    "device = 'cuda' if torch.cuda.is_available() else ('mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu')\n",
    "print(\"Chosen device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c99767a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matmul done on cuda shape torch.Size([1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "# Quick smoke test\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else ('mps' if hasattr(torch.backends, 'mps') and torch.backends.mps.is_available() else 'cpu'))\n",
    "a = torch.randn(1024, 1024, device=device)\n",
    "b = torch.randn(1024, 1024, device=device)\n",
    "with torch.inference_mode():\n",
    "    c = a @ b\n",
    "print(\"Matmul done on\", device, \"shape\", c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0607c715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir /content\n",
      "Repo already present at /content/OCR\n",
      ">>> pip install -q -r /content/OCR/requirements.txt\n",
      "<<< done 2.7s rc=0\n",
      ">>> pip install -q -r /content/OCR/vlm_lab/requirements.txt\n",
      "<<< done 2.7s rc=0\n"
     ]
    }
   ],
   "source": [
    "# Clone repo if missing and install deps (quiet).\n",
    "import subprocess, time, os\n",
    "from pathlib import Path\n",
    "\n",
    "def run(cmd):\n",
    "    t0 = time.time()\n",
    "    print('>>>', ' '.join(cmd))\n",
    "    proc = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    dt = time.time() - t0\n",
    "    print(f'<<< done {dt:.1f}s rc={proc.returncode}')\n",
    "    if proc.stdout:\n",
    "        print('stdout:', proc.stdout[:400])\n",
    "    if proc.stderr:\n",
    "        print('stderr:', proc.stderr[:400])\n",
    "\n",
    "repo_root = Path('/content/OCR')\n",
    "print('Working dir', os.getcwd())\n",
    "if not (repo_root / 'vlm_lab' / 'run_poc.py').exists():\n",
    "    run(['git', 'clone', 'https://github.com/smidolt/x.git', str(repo_root)])\n",
    "else:\n",
    "    print('Repo already present at', repo_root)\n",
    "\n",
    "run(['pip', 'install', '-q', '-r', str(repo_root / 'requirements.txt')])\n",
    "run(['pip', 'install', '-q', '-r', str(repo_root / 'vlm_lab' / 'requirements.txt')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bf59634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found documents: ['dummy_invoice.png']\n"
     ]
    }
   ],
   "source": [
    "# Ensure documents exist (dummy if empty)\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageDraw\n",
    "repo_root = Path('/content/OCR')\n",
    "input_dir = repo_root / 'data' / 'input'\n",
    "input_dir.mkdir(parents=True, exist_ok=True)\n",
    "files = list(input_dir.glob('*'))\n",
    "if not files:\n",
    "    img = Image.new('RGB', (800, 1100), 'white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    draw.text((50, 50), 'DUMMY INVOICE\\nSeller: Example Corp\\nTotal: 123.45', fill='black')\n",
    "    img_path = input_dir / 'dummy_invoice.png'\n",
    "    img.save(img_path)\n",
    "    print('Created dummy invoice at', img_path)\n",
    "else:\n",
    "    print('Found documents:', [p.name for p in files])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82fa7bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running: python /content/OCR/vlm_lab/run_poc.py --documents /content/OCR/data/input --models-file /content/OCR/vlm_lab/models.yaml --output-dir /content/OCR/vlm_lab/results --max-pages 1 --device cuda\n",
      "STDOUT:\n",
      " === Model phi3-vision (microsoft/Phi-3-vision-128k-instruct) on device cuda ===\n",
      "[ERROR] Failed to load phi3-vision: FlashAttention2 has been toggled on, but it cannot be used due to the following error: the package flash_attn seems to be not installed. Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\n",
      "=== Model llava-mistral (llava-hf/llava-v1.6-mistral-7b-hf) on device cuda ===\n",
      "[ERROR] Failed to load llava-mistral: Unrecognized configuration class <class 'transformers.models.llava_next.configuration_llava_next.LlavaNextConfig'> for this kind of AutoModel: AutoModelForCausalLM.\n",
      "Model type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config, Phi3VConfig.\n",
      "Run completed. Results stored in /content/OCR/vlm_lab/results/20251126-130340\n",
      "\n",
      "STDERR:\n",
      " 2025-11-26 13:03:33.618602: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-11-26 13:03:33.636212: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764162213.657889   48107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764162213.664442   48107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1764162213.680974   48107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764162213.680999   48107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764162213.681002   48107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1764162213.681005   48107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-11-26 13:03:33.685778: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/models/auto/image_processing_auto.py:647: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run VLM PoC\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "repo_root = Path('/content/OCR')\n",
    "repo = repo_root / 'vlm_lab'\n",
    "python_bin = repo / '.venv/bin/python'\n",
    "python_exec = python_bin if python_bin.exists() else 'python'\n",
    "models_file = repo / 'models.yaml'\n",
    "documents = repo_root / 'data' / 'input'\n",
    "output_dir = repo / 'results'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'auto'\n",
    "\n",
    "cmd = [\n",
    "    str(python_exec), str(repo / 'run_poc.py'),\n",
    "    '--documents', str(documents),\n",
    "    '--models-file', str(models_file),\n",
    "    '--output-dir', str(output_dir),\n",
    "    '--max-pages', '1',\n",
    "    '--device', device,\n",
    "]\n",
    "\n",
    "print('Running:', ' '.join(cmd))\n",
    "result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "print('STDOUT:\\n', result.stdout)\n",
    "print('STDERR:\\n', result.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dba43bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listing /content/OCR:\n",
      "Latest VLM run: 20251126-130340\n",
      "Archive ready at /content/OCR/vlm_results_20251126-130340.zip\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "download(\"download_1062f708-1ada-4cd8-be75-f69296c4e2e7\", \"vlm_results_20251126-130340.zip\", 1404)",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bundle latest VLM results for download\n",
    "import shutil, subprocess\n",
    "from pathlib import Path\n",
    "repo_root = Path('/content/OCR')\n",
    "results_root = repo_root / 'vlm_lab' / 'results'\n",
    "\n",
    "def latest_run(root: Path):\n",
    "    if not root.exists():\n",
    "        return None\n",
    "    runs = sorted([p for p in root.iterdir() if p.is_dir()], reverse=True)\n",
    "    return runs[0] if runs else None\n",
    "\n",
    "latest = latest_run(results_root)\n",
    "print('Listing /content/OCR:')\n",
    "subprocess.run(['ls', '-l', str(repo_root)])\n",
    "if latest:\n",
    "    print('Latest VLM run:', latest.name)\n",
    "    subprocess.run(['ls', '-l', str(latest)])\n",
    "    zip_path = repo_root / f'vlm_results_{latest.name}.zip'\n",
    "    shutil.make_archive(zip_path.with_suffix(''), 'zip', latest)\n",
    "    print('Archive ready at', zip_path)\n",
    "    try:\n",
    "        from google.colab import files\n",
    "        files.download(str(zip_path))\n",
    "    except Exception as e:\n",
    "        print('Download manually:', zip_path, 'error:', e)\n",
    "else:\n",
    "    print('No VLM results found in', results_root)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
